#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Posts to JSON Converter
å°† _posts ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„ JSON æ•°æ®
ä½¿ç”¨æ–¹æ³•:
  python convert_posts_to_json.py --input _posts --output apidata/posts_data.json
  python convert_posts_to_json.py --input _posts --output apidata/posts_data.json --validate
  python convert_posts_to_json.py --input _posts --output apidata/posts_data.json --single "ATP-aptamer.md"
"""
import re
import json
import yaml
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from bs4 import BeautifulSoup
import logging
class PostsToJsonConverter:
    def __init__(self, input_dir: str, output_file: str):
        self.input_dir = Path(input_dir)
        self.output_file = Path(output_file)
        self.logger = self._setup_logging()
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.section_pattern = re.compile(r'<p class="header_box" id="([^"]+)"[^>]*>([^<]+)</p>', re.IGNORECASE)
        self.subsection_pattern = re.compile(r'<p class="blowheader_box"[^>]*>([^<]+)</p>', re.IGNORECASE)
        self.image_pattern = re.compile(r'<img[^>]*src="([^"]+)"[^>]*(?:alt="([^"]*)")?[^>]*/?>', re.IGNORECASE)
        self.sequence_pattern = re.compile(r"5[''']-[AUCGT\s-]+-3[''']", re.IGNORECASE)
        self.front_matter_pattern = re.compile(r'^---\s*\n(.*?)\n---\s*\n', re.DOTALL)
        
        # é¢„å®šä¹‰çš„èŠ‚ç‚¹ç»“æ„
        self.expected_sections = {
            'timeline': 'Timeline',
            'description': 'Description', 
            'selex': 'SELEX',
            'structure': 'Structure',
            'ligand-recognition': 'Ligand information',
            'references': 'References'
        }
        
        # ç»“æ„åŒºå—çš„å­èŠ‚ç‚¹
        self.structure_subsections = [
            '2D representation',
            '3D visualisation', 
            '3D visualization',
            'Binding pocket'
        ]
        
        # é…ä½“ä¿¡æ¯åŒºå—çš„å­èŠ‚ç‚¹
        self.ligand_subsections = [
            'SELEX ligand',
            'Structure ligand', 
            'Similar compound(s)',
            'Similar compounds'
        ]
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('conversion.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    def extract_front_matter(self, content: str) -> Tuple[Dict[str, Any], str]:
        """æå–å¹¶è§£æFront Matter"""
        match = self.front_matter_pattern.match(content)
        if not match:
            return {}, content
        
        try:
            front_matter = yaml.safe_load(match.group(1))
            remaining_content = content[match.end():]
            return front_matter or {}, remaining_content
        except yaml.YAMLError as e:
            self.logger.warning(f"YAMLè§£æé”™è¯¯: {e}")
            return {}, content
    def extract_images(self, content: str) -> List[Dict[str, str]]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        for match in self.image_pattern.finditer(content):
            src = match.group(1)
            alt = match.group(2) if match.group(2) else ""
            
            # è§£æå›¾ç‰‡ç±»å‹å’Œæè¿°
            image_info = {
                'src': src,
                'alt': alt,
                'type': self._classify_image(src),
                'filename': Path(src).name if src else ""
            }
            images.append(image_info)
        
        return images
    def _classify_image(self, src: str) -> str:
        """æ ¹æ®è·¯å¾„åˆ†ç±»å›¾ç‰‡ç±»å‹"""
        if '/2D/' in src:
            return '2d_structure'
        elif '/3D/' in src:
            return '3d_structure'
        elif '/Binding_pocket/' in src:
            return 'binding_pocket'
        elif '/SELEX_ligand/' in src:
            return 'selex_ligand'
        elif '/Structure_ligand/' in src:
            return 'structure_ligand'
        elif '/Similar_compound/' in src:
            return 'similar_compound'
        else:
            return 'other'
    def extract_sequences(self, content: str) -> List[str]:
        """æå–RNA/DNAåºåˆ—"""
        sequences = []
        for match in self.sequence_pattern.finditer(content):
            sequence = match.group(0).strip()
            # æ¸…ç†ç©ºæ ¼å’Œæ ¼å¼å­—ç¬¦
            cleaned_sequence = re.sub(r'\s+', '', sequence)
            sequences.append(cleaned_sequence)
        
        return list(set(sequences))  # å»é‡
    def extract_tables(self, content: str) -> List[Dict[str, Any]]:
        """æå–è¡¨æ ¼æ•°æ®"""
        tables = []
        soup = BeautifulSoup(content, 'html.parser')
        
        for table in soup.find_all('table'):
            table_data = {
                'headers': [],
                'rows': [],
                'class': table.get('class', []),
                'style': table.get('style', '')
            }
            
            # æå–è¡¨å¤´
            thead = table.find('thead')
            if thead:
                header_row = thead.find('tr')
                if header_row:
                    for th in header_row.find_all('th'):
                        table_data['headers'].append(th.get_text(strip=True))
            
            # æå–è¡¨æ ¼æ•°æ®
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr'):
                row = []
                for td in tr.find_all(['td', 'th']):
                    cell_data = {
                        'text': td.get_text(strip=True),
                        'html': str(td),
                        'links': []
                    }
                    
                    # æå–é“¾æ¥
                    for a in td.find_all('a'):
                        href = a.get('href', '')
                        link_text = a.get_text(strip=True)
                        cell_data['links'].append({
                            'url': href,
                            'text': link_text
                        })
                    
                    row.append(cell_data)
                
                if row:  # åªæ·»åŠ éç©ºè¡Œ
                    table_data['rows'].append(row)
            
            if table_data['headers'] or table_data['rows']:
                tables.append(table_data)
        
        return tables
    def extract_references(self, content: str) -> List[Dict[str, str]]:
        """æå–å‚è€ƒæ–‡çŒ®"""
        references = []
        
        # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®åŒºå—
        ref_pattern = re.compile(r'<a id="ref(\d+)"></a>.*?<font><strong>\[(\d+)\]\s*([^<]+)</strong></font><br\s*/?>\s*([^<]+)<br\s*/?>\s*<a[^>]*>([^<]+)</a>', re.DOTALL | re.IGNORECASE)
        
        for match in ref_pattern.finditer(content):
            ref_id = match.group(1)
            ref_num = match.group(2)
            title = match.group(3).strip()
            authors = match.group(4).strip()
            journal_info = match.group(5).strip()
            
            references.append({
                'id': ref_id,
                'number': ref_num,
                'title': title,
                'authors': authors,
                'journal': journal_info
            })
        
        return references
    def extract_timeline(self, content: str) -> List[Dict[str, str]]:
        """æå–æ—¶é—´çº¿æ•°æ®"""
        timeline = []
        
        # æŸ¥æ‰¾æ—¶é—´çº¿æ¡ç›®
        entry_pattern = re.compile(r'<div class="entry">.*?<h3><a[^>]*>(\d{4})</a></h3>.*?<p>([^<]+)</p>.*?</div>', re.DOTALL | re.IGNORECASE)
        
        for match in entry_pattern.finditer(content):
            year = match.group(1)
            description = match.group(2).strip()
            
            timeline.append({
                'year': year,
                'description': description
            })
        
        return timeline
    def extract_3d_structure_info(self, content: str) -> Dict[str, Any]:
        """æå–3Dç»“æ„ç›¸å…³ä¿¡æ¯"""
        structure_info = {
            'pdb_info': {},
            'molstar_config': {},
            'color_schemes': [],
            'interactive_controls': []
        }
        
        # æå–PDBä¿¡æ¯
        pdb_id_pattern = re.compile(r'PDB ID.*?is\s+(\w+)', re.IGNORECASE)
        pdb_id_match = pdb_id_pattern.search(content)
        if pdb_id_match:
            structure_info['pdb_info']['pdb_id'] = pdb_id_match.group(1)
        
        # æå–PDBæ–‡ä»¶è·¯å¾„
        pdb_file_pattern = re.compile(r'url:\s*[\'"]([^\'\"]*\.pdb)[\'"]', re.IGNORECASE)
        pdb_file_match = pdb_file_pattern.search(content)
        if pdb_file_match:
            structure_info['pdb_info']['pdb_file_path'] = pdb_file_match.group(1)
        
        # æå–Molstaré…ç½®é€‰é¡¹
        molstar_options = {}
        
        # èƒŒæ™¯é¢œè‰²
        bg_color_pattern = re.compile(r'bgColor:\s*\{([^}]+)\}', re.IGNORECASE)
        bg_color_match = bg_color_pattern.search(content)
        if bg_color_match:
            molstar_options['background_color'] = bg_color_match.group(1)
        
        # éšè—æ§ä»¶
        hide_controls_pattern = re.compile(r'hideCanvasControls:\s*\[([^\]]+)\]', re.IGNORECASE)
        hide_controls_match = hide_controls_pattern.search(content)
        if hide_controls_match:
            controls = [ctrl.strip().strip('\'"') for ctrl in hide_controls_match.group(1).split(',')]
            molstar_options['hidden_controls'] = controls
        
        structure_info['molstar_config'] = molstar_options
        
        # æå–é¢œè‰²é€‰æ‹©è§„åˆ™
        color_selection_pattern = re.compile(r'var\s+selectSections\d+\s*=\s*\[(.*?)\]', re.DOTALL | re.IGNORECASE)
        color_selection_match = color_selection_pattern.search(content)
        if color_selection_match:
            color_rules_text = color_selection_match.group(1)
            color_rules = self._parse_color_rules(color_rules_text)
            structure_info['color_schemes'] = color_rules
        
        # æå–äº¤äº’æ§åˆ¶æŒ‰é’®
        button_pattern = re.compile(r'<button[^>]*onclick="([^"]*)"[^>]*>([^<]+)</button>', re.IGNORECASE)
        for button_match in button_pattern.finditer(content):
            onclick_code = button_match.group(1).strip()
            button_text = button_match.group(2).strip()
            structure_info['interactive_controls'].append({
                'text': button_text,
                'action': 'color_selection' if 'select' in onclick_code.lower() else 'clear_selection'
            })
        
        return structure_info
    def _parse_color_rules(self, color_rules_text: str) -> List[Dict[str, Any]]:
        """è§£æé¢œè‰²è§„åˆ™JavaScriptä»£ç """
        color_rules = []
        
        # åŒ¹é…æ¯ä¸ªé¢œè‰²è§„åˆ™å¯¹è±¡ï¼ˆå¤„ç†åµŒå¥—çš„å¤§æ‹¬å·ï¼‰
        rule_pattern = re.compile(r'\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}', re.DOTALL)
        
        for rule_match in rule_pattern.finditer(color_rules_text):
            rule_text = rule_match.group(1)
            rule_data = {}
            
            # æå–å„ä¸ªå±æ€§
            properties = [
                ('struct_asym_id', r'struct_asym_id:\s*[\'"]([^\'"]+)[\'"]'),
                ('start_residue_number', r'start_residue_number:\s*(\d+)'),
                ('end_residue_number', r'end_residue_number:\s*(\d+)'),
                ('color', r'color:\s*\{([^}]+)\}')
            ]
            
            for prop_name, pattern in properties:
                match = re.search(pattern, rule_text)
                if match:
                    if prop_name == 'color':
                        # è§£æRGBé¢œè‰²å€¼
                        color_text = match.group(1)
                        rgb_match = re.findall(r'([rgb]):\s*(\d+)', color_text)
                        if rgb_match:
                            color_dict = {component: int(value) for component, value in rgb_match}
                            rule_data[prop_name] = color_dict
                    elif prop_name in ['start_residue_number', 'end_residue_number']:
                        rule_data[prop_name] = int(match.group(1))
                    else:
                        rule_data[prop_name] = match.group(1)
            
            if rule_data:  # åªæ·»åŠ éç©ºçš„è§„åˆ™
                color_rules.append(rule_data)
        
        return color_rules
    def parse_section_content(self, section_id: str, content: str) -> Dict[str, Any]:
        """è§£æç‰¹å®šèŠ‚ç‚¹çš„å†…å®¹"""
        section_data = {
            'id': section_id,
            'content': {
                'text': '',
                'images': [],
                'tables': [],
                'sequences': [],
                'subsections': {}
            }
        }
        
        if section_id == 'timeline':
            section_data['content']['timeline_entries'] = self.extract_timeline(content)
        elif section_id == 'references':
            section_data['content']['references'] = self.extract_references(content)
        
        # æå–é€šç”¨å†…å®¹
        section_data['content']['images'] = self.extract_images(content)
        section_data['content']['tables'] = self.extract_tables(content)
        section_data['content']['sequences'] = self.extract_sequences(content)
        
        # æå–çº¯æ–‡æœ¬ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
        soup = BeautifulSoup(content, 'html.parser')
        section_data['content']['text'] = soup.get_text(separator=' ', strip=True)
        
        # å¤„ç†å­èŠ‚ç‚¹ï¼ˆä½¿ç”¨ä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…ï¼‰
        if section_id.lower() == 'structure':
            section_data['content']['subsections'] = self._extract_subsections(content, self.structure_subsections)
        elif section_id.lower() == 'ligand-recognition':
            section_data['content']['subsections'] = self._extract_subsections(content, self.ligand_subsections)
        
        return section_data
    def _extract_subsections(self, content: str, subsection_names: List[str]) -> Dict[str, Dict[str, Any]]:
        """æå–å­èŠ‚ç‚¹å†…å®¹"""
        subsections = {}
        
        # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰çš„blowheader_boxä½ç½®
        all_blowheader_pattern = re.compile(r'<p class="blowheader_box"[^>]*>([^<]+)</p>', re.IGNORECASE)
        all_matches = list(all_blowheader_pattern.finditer(content))
        
        # è°ƒè¯•ï¼šè®°å½•æ‰¾åˆ°çš„æ‰€æœ‰å­èŠ‚ç‚¹æ ‡é¢˜
        found_titles = [match.group(1).strip() for match in all_matches]
        self.logger.debug(f"Found blowheader_box titles: {found_titles}")
        self.logger.debug(f"Expected subsection names: {subsection_names}")
        
        for subsection_name in subsection_names:
            # æŸ¥æ‰¾åŒ¹é…çš„å­èŠ‚
            matched = False
            for i, match in enumerate(all_matches):
                found_title = match.group(1).strip()
                
                # çµæ´»åŒ¹é…å­èŠ‚åç§°ï¼ˆå¤„ç†å˜ä½“ï¼‰
                if self._subsection_names_match(subsection_name, found_title):
                    self.logger.debug(f"Matched '{subsection_name}' with '{found_title}'")
                    matched = True
                    
                    # ç¡®å®šå†…å®¹èŒƒå›´
                    start_pos = match.end()
                    
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªblowheader_boxæˆ–header_boxçš„ä½ç½®ä½œä¸ºç»“æŸ
                    if i + 1 < len(all_matches):
                        end_pos = all_matches[i + 1].start()
                    else:
                        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªheader_box
                        next_header_pattern = re.compile(r'<p class="header_box"', re.IGNORECASE)
                        next_header_match = next_header_pattern.search(content, start_pos)
                        if next_header_match:
                            end_pos = next_header_match.start()
                        else:
                            end_pos = len(content)
                    
                    subsection_content = content[start_pos:end_pos].strip()
                    self.logger.debug(f"Subsection '{found_title}' content length: {len(subsection_content)}")
                    
                    subsection_data = {
                        'title': found_title,
                        'text': '',
                        'images': self.extract_images(subsection_content),
                        'tables': self.extract_tables(subsection_content),
                        'sequences': self.extract_sequences(subsection_content)
                    }
                    
                    # ç‰¹æ®Šå¤„ç†3Då¯è§†åŒ–å­èŠ‚
                    if '3d' in found_title.lower() and 'visuali' in found_title.lower():
                        self.logger.debug(f"Processing 3D visualization subsection: {found_title}")
                        structure_3d_info = self.extract_3d_structure_info(subsection_content)
                        subsection_data.update(structure_3d_info)
                        self.logger.debug(f"3D structure info extracted: {structure_3d_info}")
                    
                    # æå–çº¯æ–‡æœ¬
                    soup = BeautifulSoup(subsection_content, 'html.parser')
                    subsection_data['text'] = soup.get_text(separator=' ', strip=True)
                    
                    # ä½¿ç”¨æ ‡å‡†åŒ–çš„é”®å
                    key_name = subsection_name.lower().replace(' ', '_')
                    subsections[key_name] = subsection_data
                    break
            
            if not matched:
                self.logger.warning(f"Could not find subsection '{subsection_name}' in content")
        
        return subsections
    
    def _subsection_names_match(self, expected: str, found: str) -> bool:
        """æ£€æŸ¥å­èŠ‚åç§°æ˜¯å¦åŒ¹é…ï¼ˆå¤„ç†å˜ä½“ï¼‰"""
        expected_lower = expected.lower().strip()
        found_lower = found.lower().strip()
        
        # ç›´æ¥åŒ¹é…
        if expected_lower == found_lower:
            return True
        
        # å¤„ç†å¸¸è§å˜ä½“
        variants = {
            '3d visualisation': ['3d visualization', '3d visualisation', '3d structure'],
            '3d visualization': ['3d visualisation', '3d visualization', '3d structure'],
            'similar compound(s)': ['similar compounds', 'similar compound(s)', 'similar compound'],
            'similar compounds': ['similar compound(s)', 'similar compounds', 'similar compound']
        }
        
        if expected_lower in variants:
            return found_lower in variants[expected_lower]
        
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
        import string
        expected_clean = expected_lower.translate(str.maketrans('', '', string.punctuation))
        found_clean = found_lower.translate(str.maketrans('', '', string.punctuation))
        
        return expected_clean == found_clean
    def parse_markdown_file(self, file_path: Path) -> Dict[str, Any]:
        """è§£æå•ä¸ªMarkdownæ–‡ä»¶"""
        self.logger.info(f"è§£ææ–‡ä»¶: {file_path.name}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {}
        
        # æå–Front Matter
        front_matter, main_content = self.extract_front_matter(content)
        
        # åˆå§‹åŒ–æ–‡ä»¶æ•°æ®ç»“æ„
        file_data = {
            'filename': file_path.name,
            'metadata': {
                'front_matter': front_matter,
                'file_size': file_path.stat().st_size,
                'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                'parsed_at': datetime.now().isoformat()
            },
            'sections': {}
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰èŠ‚ç‚¹
        current_pos = 0
        sections_found = []
        
        for match in self.section_pattern.finditer(main_content):
            section_id = match.group(1)
            section_title = match.group(2).strip()
            start_pos = match.start()
            
            sections_found.append({
                'id': section_id,
                'title': section_title,
                'start': start_pos,
                'match': match
            })
        
        # æå–æ¯ä¸ªèŠ‚ç‚¹çš„å†…å®¹
        for i, section in enumerate(sections_found):
            section_id = section['id']
            section_start = section['match'].end()
            
            # ç¡®å®šèŠ‚ç‚¹ç»“æŸä½ç½®
            if i + 1 < len(sections_found):
                section_end = sections_found[i + 1]['start']
            else:
                section_end = len(main_content)
            
            # æå–èŠ‚ç‚¹å†…å®¹
            section_content = main_content[section_start:section_end].strip()
            
            # è§£æèŠ‚ç‚¹
            if section_content:
                file_data['sections'][section_id] = self.parse_section_content(section_id, section_content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        file_data['metadata']['statistics'] = {
            'total_sections': len(file_data['sections']),
            'total_images': sum(len(section['content']['images']) for section in file_data['sections'].values()),
            'total_tables': sum(len(section['content']['tables']) for section in file_data['sections'].values()),
            'total_sequences': sum(len(section['content']['sequences']) for section in file_data['sections'].values()),
            'sections_found': list(file_data['sections'].keys())
        }
        
        self.logger.info(f"æ–‡ä»¶è§£æå®Œæˆ: {file_path.name} - {file_data['metadata']['statistics']['total_sections']} ä¸ªèŠ‚ç‚¹")
        return file_data
    def convert_all_posts(self, single_file: Optional[str] = None) -> Dict[str, Any]:
        """è½¬æ¢æ‰€æœ‰å¸–å­æˆ–å•ä¸ªæ–‡ä»¶"""
        results = {
            'metadata': {
                'conversion_date': datetime.now().isoformat(),
                'source_directory': str(self.input_dir),
                'total_files': 0,
                'successful_conversions': 0,
                'failed_conversions': 0,
                'errors': []
            },
            'posts': {}
        }
        
        # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶
        if single_file:
            files_to_process = [self.input_dir / single_file]
        else:
            files_to_process = list(self.input_dir.glob('*.md'))
        
        results['metadata']['total_files'] = len(files_to_process)
        
        for file_path in files_to_process:
            if not file_path.exists():
                error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                self.logger.error(error_msg)
                results['metadata']['errors'].append(error_msg)
                results['metadata']['failed_conversions'] += 1
                continue
            
            try:
                file_data = self.parse_markdown_file(file_path)
                if file_data:
                    # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰ä½œä¸ºé”®
                    file_key = file_path.stem
                    results['posts'][file_key] = file_data
                    results['metadata']['successful_conversions'] += 1
                else:
                    results['metadata']['failed_conversions'] += 1
            
            except Exception as e:
                error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}"
                self.logger.error(error_msg)
                results['metadata']['errors'].append(error_msg)
                results['metadata']['failed_conversions'] += 1
        
        return results
    def save_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜è½¬æ¢ç»“æœ"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå¤‡ä»½
        if self.output_file.exists():
            backup_file = self.output_file.with_suffix('.bak')
            import shutil
            shutil.copy2(self.output_file, backup_file)
            self.logger.info(f"å·²åˆ›å»ºå¤‡ä»½: {backup_file}")
        
        # ä¿å­˜ç»“æœï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„JSONç¼–ç å™¨å¤„ç†datetimeå¯¹è±¡
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        self.logger.info(f"è½¬æ¢ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")
        self.logger.info(f"æˆåŠŸè½¬æ¢: {results['metadata']['successful_conversions']} ä¸ªæ–‡ä»¶")
        self.logger.info(f"è½¬æ¢å¤±è´¥: {results['metadata']['failed_conversions']} ä¸ªæ–‡ä»¶")
    def validate_results(self, results: Dict[str, Any]) -> bool:
        """éªŒè¯è½¬æ¢ç»“æœ"""
        self.logger.info("å¼€å§‹éªŒè¯è½¬æ¢ç»“æœ...")
        
        validation_passed = True
        
        for post_key, post_data in results['posts'].items():
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'metadata' not in post_data:
                self.logger.error(f"{post_key}: ç¼ºå°‘ metadata å­—æ®µ")
                validation_passed = False
            
            if 'sections' not in post_data:
                self.logger.error(f"{post_key}: ç¼ºå°‘ sections å­—æ®µ")
                validation_passed = False
                continue
            
            # æ£€æŸ¥å¸¸è§èŠ‚ç‚¹
            expected_sections = ['description', 'structure', 'references']
            for section in expected_sections:
                if section not in post_data['sections']:
                    self.logger.warning(f"{post_key}: ç¼ºå°‘å¸¸è§èŠ‚ç‚¹ '{section}'")
            
            # éªŒè¯å›¾ç‰‡è·¯å¾„
            for section_id, section_data in post_data['sections'].items():
                if 'content' in section_data and 'images' in section_data['content']:
                    for image in section_data['content']['images']:
                        if not image.get('src'):
                            self.logger.warning(f"{post_key}.{section_id}: å‘ç°ç©ºå›¾ç‰‡è·¯å¾„")
        
        if validation_passed:
            self.logger.info("âœ… éªŒè¯é€šè¿‡")
        else:
            self.logger.error("âŒ éªŒè¯å¤±è´¥")
        
        return validation_passed
def main():
    parser = argparse.ArgumentParser(description='å°† _posts ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶è½¬æ¢ä¸º JSON')
    parser.add_argument('--input', '-i', default='_posts', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output', '-o', default='apidata/posts_data.json', help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--single', '-s', help='åªè½¬æ¢å•ä¸ªæ–‡ä»¶')
    parser.add_argument('--validate', '-v', action='store_true', help='è½¬æ¢åéªŒè¯ç»“æœ')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆå§‹åŒ–è½¬æ¢å™¨
    converter = PostsToJsonConverter(args.input, args.output)
    
    # æ‰§è¡Œè½¬æ¢
    results = converter.convert_all_posts(args.single)
    
    # ä¿å­˜ç»“æœ
    converter.save_results(results)
    
    # éªŒè¯ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.validate:
        converter.validate_results(results)
    
    print(f"\nğŸ‰ è½¬æ¢å®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"âœ… æˆåŠŸ: {results['metadata']['successful_conversions']} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {results['metadata']['failed_conversions']} ä¸ªæ–‡ä»¶")
    
    if results['metadata']['errors']:
        print(f"\nâš ï¸  é”™è¯¯ä¿¡æ¯:")
        for error in results['metadata']['errors']:
            print(f"   {error}")
if __name__ == "__main__":
    main() 